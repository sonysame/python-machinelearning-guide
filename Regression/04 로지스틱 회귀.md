# 로지스틱 회귀(Logistic Regression)

* 선형회귀방식을 분류에 적용한 알고리즘
* 시그모이드(sigmoid)함수 최적선을 찾고, 이 시그모이드 함수의 반환값을 확률로 간주해 확률에 따라 분류
* 가볍고 빠르며, 이진 분류 예측 성능도 뛰어남, 희소한 데이터 세트 분류에도 뛰어난 성능을 보임
* 하지만 요즘은 ensemble에 치이고 있다..


```python
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline

from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, roc_auc_score

cancer=load_breast_cancer()
scaler=StandardScaler()
data_scaled=scaler.fit_transform(cancer.data)
#data_scaled=pd.DataFrame(data_scaled)
X_train,X_test,y_train,y_test=train_test_split(data_scaled, cancer.target, test_size=0.3, random_state=0)
lr_clf=LogisticRegression(solver='liblinear') #solver가 liblinear일때 L1 penalty 가능
lr_clf.fit(X_train,y_train)
lr_preds=lr_clf.predict(X_test)
print('accuracy: {:0.3f}'.format(accuracy_score(y_test, lr_preds)))
print('roc_auc: {:0.3f}'.format(roc_auc_score(y_test , lr_preds)))

```

    accuracy: 0.982
    roc_auc: 0.979
    

### LogisticRegression 클래스의 주요 하이퍼 파라미터
* **penalty**:l1, l2
* **C**:alpha값의 역수, C값이 작을수록 규제 강도가 크다


```python
from sklearn.model_selection import GridSearchCV

params={'penalty':['l2','l1'],
       'C':[0.01,0.1,1,5,10]}

grid_clf=GridSearchCV(lr_clf,param_grid=params,scoring='accuracy',cv=3)
grid_clf.fit(data_scaled, cancer.target)
print('최적 하이퍼 파라미터:{0}, 최적 평균 정확도:{1:.3f}'.format(grid_clf.best_params_, grid_clf.best_score_))
```

    최적 하이퍼 파라미터:{'C': 0.1, 'penalty': 'l2'}, 최적 평균 정확도:0.979
    
